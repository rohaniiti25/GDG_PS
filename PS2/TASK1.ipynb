{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed299ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# DDIM ONLY DIFFUSION MODEL â€” MNIST\n",
    "# Proper U-Net + Residual Blocks + Correct DDIM Sampling\n",
    "# =========================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. SINUSOIDAL TIME EMBEDDING\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def time_embedding(t, dim):\n",
    "    half = dim // 2\n",
    "    freqs = torch.exp(\n",
    "        -math.log(10000) * torch.arange(half, device=t.device) / half\n",
    "    )\n",
    "    args = t[:, None] * freqs[None]\n",
    "    return torch.cat([torch.sin(args), torch.cos(args)], dim=1)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. RESIDUAL BLOCK\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, time_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.time_mlp = nn.Linear(time_dim, out_ch)\n",
    "\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.GroupNorm(8, out_ch),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.GroupNorm(8, out_ch),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "\n",
    "        self.shortcut = (\n",
    "            nn.Conv2d(in_ch, out_ch, 1)\n",
    "            if in_ch != out_ch else nn.Identity()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        h = self.block1(x)\n",
    "        h = h + self.time_mlp(t)[:, :, None, None]\n",
    "        h = self.block2(h)\n",
    "        return h + self.shortcut(x)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. U-NET (FIXED)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, time_dim=128):\n",
    "        super().__init__()\n",
    "\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(time_dim, time_dim * 4),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_dim * 4, time_dim)\n",
    "        )\n",
    "\n",
    "        self.in_conv = nn.Conv2d(1, 64, 3, padding=1)\n",
    "\n",
    "        self.down1 = ResBlock(64, 128, time_dim)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.down2 = ResBlock(128, 256, time_dim)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.mid = ResBlock(256, 256, time_dim)\n",
    "\n",
    "        self.up2 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
    "        self.up_block2 = ResBlock(384, 128, time_dim)  # FIXED\n",
    "\n",
    "        self.up1 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "        self.up_block1 = ResBlock(192, 64, time_dim)   # FIXED\n",
    "\n",
    "        self.out = nn.Conv2d(64, 1, 1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        t = self.time_mlp(time_embedding(t, 128))\n",
    "\n",
    "        x1 = self.in_conv(x)\n",
    "        d1 = self.down1(x1, t)\n",
    "        p1 = self.pool1(d1)\n",
    "\n",
    "        d2 = self.down2(p1, t)\n",
    "        p2 = self.pool2(d2)\n",
    "\n",
    "        mid = self.mid(p2, t)\n",
    "\n",
    "        u2 = self.up2(mid)\n",
    "        u2 = self.up_block2(torch.cat([u2, d2], dim=1), t)\n",
    "\n",
    "        u1 = self.up1(u2)\n",
    "        u1 = self.up_block1(torch.cat([u1, d1], dim=1), t)\n",
    "\n",
    "        return self.out(u1)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. DIFFUSION (DDIM ONLY)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "class Diffusion:\n",
    "    def __init__(self, T=1000):\n",
    "        self.T = T\n",
    "        self.beta = torch.linspace(1e-4, 0.02, T).to(device)\n",
    "        self.alpha = 1. - self.beta\n",
    "        self.alpha_bar = torch.cumprod(self.alpha, dim=0)\n",
    "\n",
    "    def add_noise(self, x0, t):\n",
    "        noise = torch.randn_like(x0)\n",
    "        a_bar = self.alpha_bar[t][:, None, None, None]\n",
    "        return torch.sqrt(a_bar) * x0 + torch.sqrt(1 - a_bar) * noise, noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def ddim_sample(self, model, steps=50, n=64):\n",
    "        times = np.linspace(0, self.T - 1, steps).astype(int)\n",
    "        x = torch.randn(n, 1, 28, 28).to(device)\n",
    "\n",
    "        for i in reversed(range(1, steps)):\n",
    "            t = torch.full((n,), times[i], device=device)\n",
    "            t_prev = torch.full((n,), times[i - 1], device=device)\n",
    "\n",
    "            eps = model(x, t)\n",
    "\n",
    "            a = self.alpha_bar[t][:, None, None, None]\n",
    "            a_prev = self.alpha_bar[t_prev][:, None, None, None]\n",
    "\n",
    "            x0 = (x - torch.sqrt(1 - a) * eps) / torch.sqrt(a)\n",
    "            x = torch.sqrt(a_prev) * x0 + torch.sqrt(1 - a_prev) * eps\n",
    "\n",
    "        return torch.clamp(x, -1, 1)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 5. DATA\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "dataset = datasets.MNIST(\"./data\", train=True, download=True, transform=transform)\n",
    "loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=128, shuffle=True, num_workers=2, pin_memory=True\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 6. TRAINING\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "model = UNet().to(device)\n",
    "diffusion = Diffusion()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for x, _ in loader:\n",
    "        x = x.to(device)\n",
    "        t = torch.randint(0, diffusion.T, (x.size(0),), device=device)\n",
    "\n",
    "        x_noisy, noise = diffusion.add_noise(x, t)\n",
    "        pred_noise = model(x_noisy, t)\n",
    "\n",
    "        loss = F.mse_loss(pred_noise, noise)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Loss: {total_loss / len(loader):.5f}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 7. DDIM SAMPLING\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "samples = diffusion.ddim_sample(model, steps=50, n=64)\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "for i in range(36):\n",
    "    plt.subplot(6,6,i+1)\n",
    "    plt.imshow(samples[i][0].cpu(), cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
